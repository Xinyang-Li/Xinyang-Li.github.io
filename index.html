 <!DOCTYPE html>

<html><head>
<title>Xinyang Li</title>

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css">
<style type="text/css">
 @import url("http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300italic,600,600italic");


body
{
	font-family: 'Source Sans Pro', sans-serif;
    background-color : #CDCDCD;
    font-size: 19px;
}
    .content
	{
    		width : 900px;
    		padding : 25px 30px;
    		margin : 25px auto;
    		background-color : #fff;
    		box-shadow: 0px 0px 10px #999;
    		border-radius: 15px; 
	}	
	table
	{
		padding: 5px;
	}
	
	table.pub_table,td.pub_td1,td.pub_td2
	{
		padding: 8px;
		width: 850px;
        border-collapse: separate;
        border-spacing: 15px;
        margin-top: -5px;
	}

	td.pub_td1
	{
		width:50px;
	}
    td.pub_td1 img
    {
        height:120px;
        width: 160px;
    }
	
	div#container
	{
		margin-left: auto;
		margin-right: auto;
		width: 820px;
		text-align: left;
		position: relative;
		background-color: #FFF;
	}
	div#DocInfo
	{
		color: #1367a7;
		height: 158px;
	}
	h4,h3,h2,h1
	{
		color: #3B3B3B;
	}
	h2
	{
		font-size:130%;
	}
	p
	{
		color: #5B5B5B;
		margin-bottom: 50px;
	}
	p.caption
	{
		color: #9B9B9B;
		text-align: left;
		width: 600px;
	}
	p.caption2
	{
		color: #9B9B9B;
		text-align: left;
		width: 800px;
	}
	#header_img
	{
		position: absolute;
		top: 0px; right: 0px;
    }
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}

    tsinghua_logo {
        position: absolute;
        left: 646px;
        top: 14px;
        width: 200px;
        height: 20px;
    }
   
    table.pub_table tr {
        outline: thin dotted #666666;
    }
    .papericon {
        border-radius: 8px; 
        -moz-box-shadow: 3px 3px 6px #888;
        -webkit-box-shadow: 3px 3px 6px #888;
        box-shadow: 3px 3px 6px #888;
    }


underline {
	border-bottom: 2px solid;
	display:inline-block;
}

</style>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-23931362-2', 'mit.edu');
  ga('send', 'pageview');

</script>
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-23931362-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

    var myPix = new Array("image/profile_correct.jpg")
    function choosePic() {
        var randomNum = Math.floor(Math.random() * myPix.length);
        document.getElementById("myPicture").src = myPix[randomNum];
    };

</script>
</head>


<body>
<div class="content">
	<div id="container">

	<table>
	<tbody><tr>
	<td><img id="myPicture" src="image/lixinyang.jpg" style="float:left; padding-right:10px" height="230px"></td>
	<td>
	<div id="DocInfo">
		<h1>Xinyang Li</h1>
        Postdoctor, Assistant Research Fellow<br>
	      Department of Automation, Tsinghua University<br>
        Institute for Brain and Cognitive Sciences, Tsinghua University<br>
        Email: xinyangli@tsinghua.edu.cn<br>
        <a href="CV-lixinyang.pdf">CV</a> &bull; <a href="https://scholar.google.com/citations?user=E1P0JY4AAAAJ&hl=en">Google Scholar</a> &bull; <a href="https://github.com/cabooster">GitHub</a>
	</td>
	</tr>
	</tbody></table>
		
	<br />
	<h2>About Me</h2>
    <ul>
        <li>I am a postdoctoral research assistant at the <a href="https://www.au.tsinghua.edu.cn/">Department of Automation</a> of <a href="https://www.tsinghua.edu.cn/">Tsinghua University</a>, working under Prof. <a href="https://www.au.tsinghua.edu.cn/info/1018/3077.htm">Qionghai Dai</a>, academician of Chinese Academy of Engineering. </li>
        <li>I received my Ph.D. degree at the <a href="https://www.au.tsinghua.edu.cn/">Department of Automation</a> of <a href="https://www.tsinghua.edu.cn/">Tsinghua University</a> in 2023, advised by Prof. <a href="https://www.au.tsinghua.edu.cn/info/1018/3077.htm">Qionghai Dai</a> and Prof. <a href="https://www.sigs.tsinghua.edu.cn/whq/">Haoqian Wang</a>. </li>
        <li>I received my bachelor's degree at the <a href="https://eie.xjtu.edu.cn/">School of Electronic and Information Engineering</a> of <a href="http://xjtu.edu.cn/">Xi'an Jiaotong University</a> in 2018. </li>
        <li>My research is on interdisciplinary research of artificial intelligence, optical imaging, and neuroscience, particularly <b>intelligent image analysis<b> and <b>intelligent imaging<b>. </li> 
	<li>I will join the <a href="https://collegeai.tsinghua.edu.cn/">College of AI at Tsinghua University</a> as an Assistant Professor in 2025. I am recruiting self-motivated students interested in AI, optical imaging, quantum optics, and robotics. </li>
    </ul>
	
    <h2>Updates</h2>
    <ul>
        <li>[2023/11/08] 1 paper is accepted by <a href="https://www.nature.com/natcomputsci">Nature Computational Science</a>!</li>
        </ul>
    <ul> 
        <li>[2023/09/29] 1 paper is accepted by <a href="https://www.nature.com/nmeth/">Nature Methods</a>!</li>
        </ul>
    <ul>
        <li>[2023/04/28] 1 invited comment is accepted by <a href="https://www.nature.com/nmeth/">Nature Methods</a>!</li>
        </ul>
    <ul>
        <li>[2023/04/10] 1 papers is accepted by <a href="https://www.cell.com/">Cell</a>!</li>
        </ul>

    <ul>
        <li>[2023/03/07] 1 paper is accepted by <a href="https://www.nature.com/nmeth/">Nature Methods</a>!</li>
        </ul>

    <ul>
        <li>[2022/11/14] 1 papers is accepted by <a href="https://photonix.springeropen.com/">PhotoniX</a>!</li>
        </ul>

    <ul>
        <li>[2022/07/29] 1 paper is accepted by <a href="https://www.nature.com/nbt/">Nature Biotechnology</a>!</li>
        </ul>

    <ul>
      <li>[2021/06/28] 1 paper is accepted by <a href="https://www.nature.com/nmeth/">Nature Methods</a>!</li>
        </ul>

	<h2>Publications</h2>
	<table class="pub_table" >
	<tbody>
        <tr>
           <td class="pub_td1"><img src="image/deepcad-rt.jpg" class="papericon"></td>
           <td class="pub_td2"><u>Xinyang Li</u>, Yixin Li, Yiliang Zhou, Jiamin Wu, Zhifeng Zhao, Jiaqi Fan, Fei Deng, Zhaofa Wu, Guihua Xiao, Jing He, Yuanlong Zhang, Guoxun Zhang, et al. <br><strong>Real-time denoising enables high-sensitivity fluorescence time-lapse imaging beyond the shot-noise limit.</strong><br><i>Nature Biotechnology</i>, 2023.<br>[<a href="https://www.nature.com/articles/s41587-022-01450-8">Link</a>][<a href="NBT-Real-time denoising enables high-sensitivi.pdf">PDF</a>][<a href="https://github.com/cabooster/DeepCAD-RT">Code</a>]
         </td>
        </tr>

         <tr>
            <td class="pub_td1"><img src="image/NM-comment-challenge.jpg" class="papericon"></td>
            <td class="pub_td2"><u>Xinyang Li</u>, Yuanlong Zhang, Jiamin Wu, and Qionghai Dai. <br><strong>Challenges and opportunities in bioimage analysis.</strong><br><i>Nature Methods</i>, 2023.<br>[<a href="https://www.nature.com/articles/s41592-023-01900-4">Link</a>][<a href="Nature Methods-2023-Challenges and opportuniti.pdf">PDF</a>]
          </td>
         </tr>

         <tr>
          <td class="pub_td1"><img src="image/NCS-SRDTrans.jpg" class="papericon"></td>
          <td class="pub_td2"><u>Xinyang Li</u>, Xiaowan Hu, Xingye Chen, Zhifeng Zhao, Jiaqi Fan, Jiamin Wu, Haoqian Wang, Qionghai Dai. <br><strong>Spatial redundancy transformer for self-supervised fluorescence image denoising.</strong><br><i>Nature Computational Science</i>, 2023.<br>[<a href="https://www.nature.com/articles/s43588-023-00568-2">Link</a>][<a href="NCS-2023-Spatial redundancy transf.pdf">PDF</a>][<a href="https://github.com/cabooster/SRDTrans">Code</a>]
        </td>
       </tr>

         <tr>
            <td class="pub_td1"><img src="image/deepcad.jpg" class="papericon"></td>
            <td class="pub_td2"><u>Xinyang Li</u>, Guoxun Zhang, Jiamin Wu, Yuanlong Zhang, Zhifeng Zhao, Xing Lin, Hui Qiao, Hao Xie, Haoqian Wang, Lu Fang, Qionghai Dai. <br><strong>Reinforcing neuron extraction and spike inference in calcium imaging using deep self-supervised denoising.</strong><br><i>Nature Methods</i>, 2021.<br>[<a href="https://www.nature.com/articles/s41592-021-01225-0">Link</a>][<a href="Nature Methods-2021-Reinforcing neuron extract.pdf">PDF</a>][<a href="https://github.com/cabooster/DeepCAD">Code</a>]
          </td>
         </tr>

         <tr>
            <td class="pub_td1"><img src="image/light-UTOM.jpg" class="papericon"></td>
            <td class="pub_td2"><u>Xinyang Li</u>, Guoxun Zhang, Hui Qiao, Feng Bao, Yue Deng, Jiamin Wu, Yangfan He, Jingping Yun, Xing Lin, Hao Xie, Haoqian Wang, Qionghai Dai. <br><strong>Real-time denoising enables high-sensitivity fluorescence time-lapse imaging beyond the shot-noise limit.</strong><br><i>Light: Science & Applications</i>, 2021.<br>[<a href="https://www.nature.com/articles/s41377-021-00484-y">Link</a>][<a href="Light_ Science-2021-Unsupervised content-prese.pdf">PDF</a>][<a href="https://github.com/cabooster/UTOM">Code</a>]
          </td>
         </tr>

         <tr>
            <td class="pub_td1"><img src="image/oe-adaps.jpg" class="papericon"></td>
            <td class="pub_td2"><u>Xinyang Li</u>, Yuanlong Zhang, Kan Liu, Hao Xie, Haoqian Wang, Lingjie Kong, Qionghai Dai. <br><strong>Adaptive optimization for axial multi-foci generation in multiphoton microscopy.</strong><br><i>Optics Express</i>, 2019.<br>[<a href="https://opg.optica.org/oe/fulltext.cfm?uri=oe-27-24-35948&id=423444">Link</a>][<a href="Opt Express-2019-Adaptive optimization for axi.pdf">PDF</a>][<a href="https://github.com/cabooster/AdaPS">Code</a>]
          </td>
         </tr>

         <tr>
            <td class="pub_td1"><img src="image/cell-2psam.jpg" class="papericon"></td>
            <td class="pub_td2">Zhifeng Zhao, Yiliang Zhou, Bo Liu, Jing He, Jiayin Zhao, Yeyi Cai, Jingtao Fan, <u>Xinyang Li</u>, Zilin Wang, Zhi Lu, Jiamin Wu, Hai Qi, Qionghai Dai. <br><strong>Two-photon synthetic aperture microscopy for minimally invasive fast 3D imaging of native subcellular behaviors in deep tissue.</strong><br><i>Cell</i>, 2023.<br>[<a href="https://www.cell.com/cell/fulltext/S0092-8674(23)00412-9">Link</a>][<a href="Cell-2023-Two-photon synthetic aperture micros.pdf">PDF</a>][<a href="https://github.com/BBNCELi/2pSAM_recon">Code</a>]
          </td>
         </tr>
         
         <tr>
            <td class="pub_td1"><img src="image/NM-deepsemi.jpg" class="papericon"></td>
            <td class="pub_td2">Guoxun Zhang, Xiaopeng Li, Yuanlong Zhang, Xiaofei Han, <u>Xinyang Li</u>, Jinqiang Yu, Boqi Liu, Jiamin Wu, Li Yu, Qionghai Dai. <br><strong>Bio-friendly long-term subcellular dynamic recording by self-supervised image enhancement microscopy.</strong><br><i>Nature Methods</i>, 2023.<br>[<a href="https://www.nature.com/articles/s41592-023-02058-9">Link</a>][<a href="Nat Methods-2023-Bio-friendly long-term subcel.pdf">PDF</a>][<a href="https://github.com/GuoxunZhang-THU/DeepSeMi">Code</a>]
          </td>
         </tr>

         <tr>
            <td class="pub_td1"><img src="image/NM-deepwonder.jpg" class="papericon"></td>
            <td class="pub_td2">Yuanlong Zhang, Guoxun Zhang, Xiaofei Han, Jiamin Wu, Ziwei Li, <u>Xinyang Li</u>, Guihua Xiao, Hao Xie, Lu Fang, Qionghai Dai. <br><strong>Rapid detection of neurons in widefield calcium imaging datasets after training with synthetic data.</strong><br><i>Nature Methods</i>, 2023.<br>[<a href="https://www.nature.com/articles/s41592-023-01838-7">Link</a>][<a href="Nat Methods-2023-Rapid detection of neurons in.pdf">PDF</a>][<a href="https://github.com/yuanlong-o/Deep_widefield_cal_inferece">Code</a>]
          </td>
         </tr>

         <tr>
            <td class="pub_td1"><img src="image/photonix.jpg" class="papericon"></td>
            <td class="pub_td2">Yi Zhang, Yuling Wang, Mingrui Wang, Yuduo Guo, <u>Xinyang Li</u>, Yifan Chen, Zhi Lu, Jiamin Wu, Xiangyang Ji, Qionghai Dai. <br><strong>Multi-focus light-field microscopy for high-speed large-volume imaging.</strong><br><i>PhotoniX</i>, 2023.<br>[<a href="https://photonix.springeropen.com/articles/10.1186/s43074-022-00076-y">Link</a>][<a href="PhotoniX-2022-Multi-focus light-field microsco.pdf">PDF</a>]
          </td>
         </tr>

         <tr>
            <td class="pub_td1"><img src="image/boe-hilo.jpg" class="papericon"></td>
            <td class="pub_td2">Ruheng Shi, Cheng Jin, Hao Xie, Yuanlong Zhang, <u>Xinyang Li</u>, Qionghai Dai, Lingjie Kong. <br><strong>Multi-plane, wide-field fluorescent microscopy for biodynamic imaging in vivo.</strong><br><i>Biomedical Optics Express</i>, 2022.<br>[<a href="https://opg.optica.org/boe/fulltext.cfm?uri=boe-10-12-6625&id=423664">Link</a>][<a href="Biomed Opt Expr-2019-Multi-plane, wide-field f.pdf">PDF</a>]
          </td>
         </tr>

         <tr>
            <td class="pub_td1"><img src="image/SR-git.jpg" class="papericon"></td>
            <td class="pub_td2">Soheil Soltani, Ashkan Ojaghi, Hui Qiao, Nischita Kaza, <u>Xinyang Li</u>, Qionghai Dai, Adeboye O. Osunkoya, Francisco E. Robles. <br><strong>Prostate cancer histopathology using label-free multispectral deep-uv microscopy quantifies phenotypes of tumor aggressiveness and enables multiple diagnostic virtual stains.</strong><br><i>Scientific Reports</i>, 2022.<br>[<a href="https://www.nature.com/articles/s41598-022-13332-9">Link</a>][<a href="Scientific Repo-2022-Prostate cancer histopath.pdf">PDF</a>]
          </td>
         </tr>

         <tr>
          <td class="pub_td1"><img src="image/congress-bio.jpg" class="papericon"></td>
          <td class="pub_td2"><u>Xinyang Li</u>, Zhifeng Zhao, Guoxun Zhang, Hui Qiao, Haoqian Wang, Qinghai Dai. <br><strong>High-fidelity fluorescence image restoration using deep unsupervised learning.</strong><br><i>OSA Biophotonics Congress: Biomedical Optics</i>, 2020.<br>[<a href="https://opg.optica.org/abstract.cfm?uri=Microscopy-2020-MW2A.2">Link</a>][<a href="microscopy-2020-mw2a.2.pdf">PDF</a>]
        </td>
       </tr>

        <tr>
          <td class="pub_td1"><img src="image/FC-CHAOWEI.jpg" class="papericon"></td>
          <td class="pub_td2">Chaowei Zhuang, <u>Xinyang Li</u>, Yuanlong Zhang, Lingjie Kong, Hao Xie, Qionghai Dai. <br><strong>Photobleaching Imprinting Enhanced Background Rejection in Line-Scanning Temporal Focusing Microscopy.</strong><br><i>Frontiers in Chemistry</i>, 2020.<br>[<a href="https://www.frontiersin.org/articles/10.3389/fchem.2020.618131/full">Link</a>][<a href="Front Chem-2020-Photobleaching Imprinting Enha.pdf">PDF</a>]
        </td>
      </tr>

        <tr>
          <td class="pub_td1"><img src="image/JPDAP.jpg" class="papericon"></td>
          <td class="pub_td2">Yuanlong Zhang*, <u>Xinyang Li</u>*, Hao Xie, Lingjie Kong, Qionghai Dai. <br><strong>Hybrid spatio-spectral coherent adaptive compensation for line-scanning temporal focusing microscopy.</strong><br><i>Journal of Physics D: Applied Physics</i>, 2019.<br>[<a href="https://iopscience.iop.org/article/10.1088/1361-6463/aae75b/meta">Link</a>][<a href="Journal of Phys-2019-Hybrid spatio-spectral co.pdf">PDF</a>]
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="image/oe-tiankuang.jpg" class="papericon"></td>
        <td class="pub_td2">Yuanlong Zhang, Tiankuang Zhou, Xuemei Hu, <u>Xinyang Li</u>, Hao Xie, Lu Fang, Lingjie Kong, Qionghai Dai. <br><strong>Overcoming tissue scattering in wide-field two-photon imaging by extended detection and computational reconstruction.</strong><br><i>Optics Express</i>, 2019.<br>[<a href="https://opg.optica.org/oe/fulltext.cfm?uri=oe-27-15-20117&id=415064">Link</a>][<a href="Opt Express-2019-Overcoming tissue scattering.pdf">PDF</a>][<a href="https://github.com/rickyim/ED_LTFM">Code</a>]
      </td>
     </tr>


	</tbody></table>

  <h2>Lectures</h2>
  <ul>
     <li><u>Xinyang Li</u>, “High-sensitivity imaging and computing beyond the standard quantum limit: from AI to quantum”, <i>College of AI at Tsinghua University</i>, Public research talk, December 2024..</li>
     <li><u>Xinyang Li</u>, “Deep self-supervised denoising enables ultrasensitive fluorescence imaging beyond the shot-noise limit”, <i>The 3rd CAAl International Conference on Artificial Intelligence</i>, invited talk on intelligent microscopy, July 2023.</li>
     <li><u>Xinyang Li</u>, “Real-time denoising of fluorescence imaging using DeepCAD-RT”, <i>Tsinghua Laboratory of Brain and Intelligence</i>, Lab Seminar, April 2023.</li>
     <li><u>Xinyang Li</u>, “Deep self-supervised denoising enables ultrasensitive fluorescence imaging beyond the shot-noise limit”, <i>Tsinghua IDG/McGovern Institute for Brain Research</i>, Seminar on Neuroscience Frontiers, September 2022.</li>
    <li><u>Xinyang Li</u>, “Reinforcing neuron extraction and spike inference in calcium imaging using deep self-supervised denoising”, <i>Peking University</i>, Lab Seminar, March 2022.</li>
    <li><u>Xinyang Li</u> and Guoxun Zhang, “High-fidelity fluorescence image restoration using deep unsupervised learning”, <i>OSA Biophotonics Congress</i>, Oral Presentation, April 2020.</li>
  </ul>

<h2>Professional Activities</h2>
        <ul>
          <li><b>Invited Speaker</b>, The 3rd CAAl International Conference on Artificial Intelligence.</li>
          <li><b>Reviewer</b>, IEEE Transactions on Circuits and Systems for Video Technology.</li>
	  <li><b>Reviewer</b>, Scientific Reports </li>
          <li><b>Reviewer</b>, Biomedical Optics Express. </li>
          <li><b>Funding</b>, China Postdoctoral Science Foundation.</li>
          <li><b>Invited Visitor</b>, The University of Hong Kong, China, May 2023.</li>
          <li><b>Invited Visitor</b>, Heidelberg University, October 2023.</li>
          <li><b>Invited Visitor</b>, Carl Zeiss AG and University of Jena, October 2023.</li>
        </ul>
      <img id="visitingPicture" src="image/visiting-photos.jpg" height="250px"></td>
      Photos with <a href="https://www.mech.hku.hk/academic-staff/huang-mx">M.X. Huang </a>, Head of the <a href="https://www.mech.hku.hk/">Department of Mechanical Engineering of HKU </a> (left); <a href="https://scholar.google.com/citations?hl=en&user=TGLiv1UAAAAJ">Jost B. Jonas </a> and <a href="https://www.praxisprof-jonas.com/">S. Panda-Jonas </a> at Heidelberg (middle); <a href="https://sites.google.com/site/heintzmann/">Rainer Heintzmann </a> besides the famous Ernst Abbe Memorial at Jena (right).
      <p style="margin:10px;"> </p>
      
    <h2>Awards and Honors:</h2>
        <ul>
	  <li>Shuimu Scholar Award, Tsinghua University, 2023.</li>
          <li>Outstanding Ph.D. Graduate of Beijing, 2023.</li>
          <li>Outstanding Ph.D. Graduate of Tsinghua University, 2023.</li>
          <li>Outstanding Doctoral Dissertation of Tsinghua University, 2023.</li>
          <li>Award for Excellent Academic Lecture, THU-IDG/McGovern Institute for Brain Research, 2023.</li>
          <li>National Scholarship, Tsinghua University, 2022.</li>
          <li>THU-IDG/McGovern Award for Outstanding Research Achievement, 2022.</li>
          <li>National Scholarship, Tsinghua University, 2021.</li>
          <li>First Prize for Outstanding Laboratory Contribution, Tsinghua University, 2021.</li>
          <li>Outstanding Graduate Student Leaders, Xi'an Jiaotong University, 2018.</li>
          <li>National Scholarship, Xi'an Jiaotong University, 2017.</li>
          <li>National Scholarship, Xi'an Jiaotong University, 2016.</li>
          <li>Outstanding Student Leaders, Xi'an Jiaotong University, 2016.</li>
          <li>National Scholarship, Xi'an Jiaotong University, 2015.</li>
        </ul>
   
  
</div>
</div>
</body></html>
